{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fb8b39-5807-4a2d-a81f-c3209f46b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/crowleyd/agaid/venv_agaid/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-23 01:58:19,794\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gymnasium as gym\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac4a4c-814c-4dc6-b690-5cc1a9682427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 01:58:25,063\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "if not ray.is_initialized():\n",
    "    ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8291a-9e2d-4aa7-9ae5-2cd43106484a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('plot_st_matrix.py', 'r') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ab5e4c-8450-4446-9faf-169f4aad276d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "46415\n",
      "100\n",
      "21544\n",
      "100000\n",
      "2154\n",
      "215\n",
      "464\n",
      "10000\n",
      "4641\n",
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = Path('/homes/crowleyd/agaid/runs/Pendulum-v1/200k_actors__test_ray')\n",
    "\n",
    "# data_size_to_model_map[data_size][model_type] = event_accumulator\n",
    "data_size_to_model_map = dict()\n",
    "for data_size_dir in rootdir.iterdir():\n",
    "    data_size = int(data_size_dir.name[len('data_size_'):])\n",
    "    model_type_to_event_accumulator = dict()\n",
    "    for model_dir in data_size_dir.iterdir():\n",
    "        model_type = model_dir.name\n",
    "        event_accumulator = EventAccumulator(model_dir)\n",
    "        model_type_to_event_accumulator[model_type] = event_accumulator\n",
    "    data_size_to_model_map[data_size] = model_type_to_event_accumulator\n",
    "\n",
    "event_acc = EventAccumulator(logdir)\n",
    "# event_acc.Reload()\n",
    "# Show all tags in the log file\n",
    "print(event_acc.Tags())\n",
    "\n",
    "# E. g. get wall clock, number of steps and value for a scalar 'Accuracy'\n",
    "# w_times, step_nums, vals = zip(*event_acc.Scalars('Accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e570bea0-d561-4ab7-b0b8-ca9afa0ae6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': [],\n",
       " 'audio': [],\n",
       " 'histograms': [],\n",
       " 'scalars': ['train/mean loss',\n",
       "  'train/state element 0/mean loss',\n",
       "  'train/state element 1/mean loss',\n",
       "  'train/state element 2/mean loss',\n",
       "  'train/task losses/mean loss',\n",
       "  'train/task losses/task_0/state element 0',\n",
       "  'train/task losses/task_0/state element 1',\n",
       "  'train/task losses/task_0/state element 2',\n",
       "  'train/task losses/task_1/state element 0',\n",
       "  'train/task losses/task_1/state element 1',\n",
       "  'train/task losses/task_1/state element 2',\n",
       "  'train/task losses/task_2/state element 0',\n",
       "  'train/task losses/task_2/state element 1',\n",
       "  'train/task losses/task_2/state element 2',\n",
       "  'train/task losses/task_3/state element 0',\n",
       "  'train/task losses/task_3/state element 1',\n",
       "  'train/task losses/task_3/state element 2',\n",
       "  'train/task losses/task_4/state element 0',\n",
       "  'train/task losses/task_4/state element 1',\n",
       "  'train/task losses/task_4/state element 2',\n",
       "  'train/task losses/task_5/state element 0',\n",
       "  'train/task losses/task_5/state element 1',\n",
       "  'train/task losses/task_5/state element 2',\n",
       "  'train/task losses/task_6/state element 0',\n",
       "  'train/task losses/task_6/state element 1',\n",
       "  'train/task losses/task_6/state element 2',\n",
       "  'train/task losses/task_7/state element 0',\n",
       "  'train/task losses/task_7/state element 1',\n",
       "  'train/task losses/task_7/state element 2',\n",
       "  'train/task losses/task_8/state element 0',\n",
       "  'train/task losses/task_8/state element 1',\n",
       "  'train/task losses/task_8/state element 2',\n",
       "  'train/task losses/task_9/state element 0',\n",
       "  'train/task losses/task_9/state element 1',\n",
       "  'train/task losses/task_9/state element 2',\n",
       "  'train/task losses/task_10/state element 0',\n",
       "  'train/task losses/task_10/state element 1',\n",
       "  'train/task losses/task_10/state element 2',\n",
       "  'train/task losses/task_11/state element 0',\n",
       "  'train/task losses/task_11/state element 1',\n",
       "  'train/task losses/task_11/state element 2',\n",
       "  'test/mean loss',\n",
       "  'test/state element 0/mean loss',\n",
       "  'test/state element 1/mean loss',\n",
       "  'test/state element 2/mean loss',\n",
       "  'test/task losses/mean loss',\n",
       "  'test/task losses/task_0/state element 0',\n",
       "  'test/task losses/task_0/state element 1',\n",
       "  'test/task losses/task_0/state element 2',\n",
       "  'test/task losses/task_1/state element 0',\n",
       "  'test/task losses/task_1/state element 1',\n",
       "  'test/task losses/task_1/state element 2',\n",
       "  'test/task losses/task_2/state element 0',\n",
       "  'test/task losses/task_2/state element 1',\n",
       "  'test/task losses/task_2/state element 2'],\n",
       " 'distributions': [],\n",
       " 'tensors': [],\n",
       " 'graph': False,\n",
       " 'meta_graph': False,\n",
       " 'run_metadata': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = ('/homes/crowleyd/agaid/runs/Pendulum-v1/200k_actors__test_ray/data_size_100/embed')\n",
    "ea = EventAccumulator(logdir).Reload()\n",
    "# ea.Reload()\n",
    "ea.Tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb0788d-59ef-4661-be99-14586c218da6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/mean loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ScalarEvent(wall_time=1714662033.6424186, step=0, value=12.817377090454102),\n",
       " ScalarEvent(wall_time=1714662065.7381232, step=1, value=7.632518291473389)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = ea.scalars.Keys()[0]\n",
    "print(key)\n",
    "ea.scalars.Items(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3c59c-e3ef-48af-8def-d859024929eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ea.scalars.FilterItems("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03d003c-6f68-4756-8823-51d51561bb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AddItem',\n",
       " 'FilterItems',\n",
       " 'Items',\n",
       " 'Keys',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_buckets',\n",
       " '_mutex',\n",
       " 'always_keep_last',\n",
       " 'size']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ea.scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b17a7-f7a0-4d59-8081-877ef1318fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "439aa48f-1dde-4674-806c-4cec70feec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10*3*1).reshape(5, 6)\n",
    "t = torch.randint(0, 3, x.size()[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f838ba37-a341-4199-a9b4-19004f108996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29]])\n",
      "\n",
      "t tensor([2, 1, 0, 1, 0])\n",
      "\n",
      "l tensor([[12, 13, 14, 15, 16, 17],\n",
      "        [24, 25, 26, 27, 28, 29]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([18., 19., 20., 21., 22., 23.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('x', x)\n",
    "print()\n",
    "print('t', t)\n",
    "print()\n",
    "l = x[t == 0]\n",
    "print('l', l)\n",
    "l.mean(dim=0, dtype=float)\n",
    "l.type(torch.float).mean(dim=0)\n",
    "# idx = torch.tensor(idx, dtype=int).unsqueeze(0)\n",
    "# torch.take_along_dim(x, idx, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3d19e-6e58-435e-a3ea-cb8d9bb5c1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dd409b6-e917-4753-b251-f60e637c4819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30],\n",
       "        [40, 50, 60]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n",
    "max_idx = torch.argmax(t)\n",
    "torch.take_along_dim(t, max_idx)\n",
    "sorted_idx = torch.argsort(t, dim=1)\n",
    "torch.take_along_dim(t, sorted_idx, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d917cf1d-177e-4923-8f6b-b90a0e22d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: tensor([[0.9227, 0.1393, 0.3681, 0.6691],\n",
      "        [0.6829, 0.2197, 0.8164, 0.6478],\n",
      "        [0.7188, 0.5352, 0.4385, 0.5393],\n",
      "        [0.1403, 0.3910, 0.8263, 0.3960],\n",
      "        [0.5600, 0.0766, 0.3439, 0.3028],\n",
      "        [0.7283, 0.7004, 0.3392, 0.9929],\n",
      "        [0.7415, 0.8326, 0.9023, 0.8434],\n",
      "        [0.6863, 0.9273, 0.9516, 0.4704],\n",
      "        [0.2948, 0.5546, 0.1300, 0.7835],\n",
      "        [0.5027, 0.6335, 0.4883, 0.5011]])\n",
      "error^2: tensor([[0.8514, 0.0194, 0.1355, 0.4477],\n",
      "        [0.4664, 0.0483, 0.6665, 0.4196],\n",
      "        [0.5167, 0.2865, 0.1923, 0.2908],\n",
      "        [0.0197, 0.1529, 0.6828, 0.1568],\n",
      "        [0.3137, 0.0059, 0.1182, 0.0917],\n",
      "        [0.5304, 0.4905, 0.1151, 0.9859],\n",
      "        [0.5498, 0.6933, 0.8141, 0.7114],\n",
      "        [0.4711, 0.8598, 0.9055, 0.2213],\n",
      "        [0.0869, 0.3076, 0.0169, 0.6139],\n",
      "        [0.2527, 0.4013, 0.2384, 0.2511]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10, 4)\n",
    "y = torch.zeros(10, 4)\n",
    "print(f\"error: {x - y}\")\n",
    "print(f\"error^2: {(x - y)**2}\")\n",
    "\n",
    "F.mse_loss(x, y, reduction='none').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f249440-f35c-46cf-b171-47cbf53f7134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n",
    "max_idx = torch.argmax(t)\n",
    "torch.take_along_dim(t, max_idx, )\n",
    "sorted_idx = torch.argsort(t, dim=1)\n",
    "torch.take_along_dim(t, sorted_idx, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4fb7569-46ef-4162-9b15-eb24bb304580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(10)\n",
    "print(t.device)\n",
    "print(t.to('cpu') is t)\n",
    "print(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d148f52-9659-4ac5-84cd-8fcf409184bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space\n",
      "class_name\n",
      "close\n",
      "env\n",
      "get_wrapper_attr\n",
      "metadata\n",
      "np_random\n",
      "observation_space\n",
      "render\n",
      "render_mode\n",
      "reset\n",
      "reward_range\n",
      "spec\n",
      "step\n",
      "unwrapped\n",
      "wrapper_spec\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=None)#\"human\")\n",
    "for e in dir(env):\n",
    "    if e[0] != '_': print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bad0e75-fa3d-4cc1-a278-271f9fa153d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14995256,  0.9886932 , -0.12224312], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, info = env.reset(seed=42)\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06a10ffe-90b8-441d-870f-0af2041c93b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_iter: 199, terminated: False, truncated: True\n",
      "global_iter: 399, terminated: False, truncated: True\n",
      "global_iter: 599, terminated: False, truncated: True\n",
      "global_iter: 799, terminated: False, truncated: True\n",
      "global_iter: 999, terminated: False, truncated: True\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")#, render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"global_iter: {_}, terminated: {terminated}, truncated: {truncated}\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ba188-ae40-4798-9a0a-fa20fcff4d6c",
   "metadata": {},
   "source": [
    "### torch.utils.data.Dataset/DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e9bb65-638d-420c-9db1-52f2e990315f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x1057f13a0>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(custom_dataset, \n\u001b[1;32m     19\u001b[0m                         batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     20\u001b[0m                         shuffle\u001b[38;5;241m=\u001b[39mshuffle, \n\u001b[1;32m     21\u001b[0m                         num_workers\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m     22\u001b[0m                         collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m batch: custom_collate_fn(batch, trajectory_sampling))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Iterate over batches\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Batch is a tuple containing input tensors\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trajectory_sampling:\n",
      "File \u001b[0;32m~/Documents/drl/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/drl/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/drl/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1035\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x1057f13a0>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import CustomDataset, custom_collate_fn\n",
    "\n",
    "# Dummy data (for demonstration purpose)\n",
    "data = torch.randn(100, 2)  # 100 samples, each with 2 features\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = CustomDataset(data)\n",
    "\n",
    "# Create a DataLoader with dynamic trajectory sampling\n",
    "batch_size = 10\n",
    "shuffle = True\n",
    "num_workers = 2\n",
    "trajectory_sampling = True  # Set to True to sample trajectories, False for individual transitions\n",
    "\n",
    "# Dynamically set trajectory sampling when creating DataLoader\n",
    "dataloader = DataLoader(custom_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=shuffle, \n",
    "                        num_workers=num_workers,\n",
    "                        collate_fn=lambda batch: custom_collate_fn(batch, trajectory_sampling))\n",
    "\n",
    "# Iterate over batches\n",
    "for batch in dataloader:\n",
    "    # Batch is a tuple containing input tensors\n",
    "    inputs = batch\n",
    "    if trajectory_sampling:\n",
    "        print(\"Trajectory batch shape:\", inputs.shape)  # shape of the batch when sampling trajectories\n",
    "    else:\n",
    "        print(\"Individual transition batch shape:\", inputs.shape)  # shape of the batch when sampling individual transitions\n",
    "    # Do whatever processing you need here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7417a41d-45b0-4273-88c3-5e1b4ba3a319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from importlib import  import_module, reload\n",
    "\n",
    "# import ipdb as pdb #  for debugging\n",
    "# from ipdb import set_trace\n",
    "import numpy as np\n",
    "\n",
    "from mt_model_buffer import MT_Model_Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea6532a-773c-4ab3-b653-d18c59850464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    env_id = 'Pendulum-v1_old'\n",
    "    buffer_name_pattern = \"min_total_steps_100000__actor__[\\w-]+\\.pkl\"\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb5f9c2-4389-4472-9125-7df616fd685a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:03<00:00, 31.58s/it]\n"
     ]
    }
   ],
   "source": [
    "path_to_agaid = Path.cwd()\n",
    "assert path_to_agaid.name == \"agaid\"\n",
    "buffer_tasks_dir = path_to_agaid / f\"offline_data/{args.env_id}\"\n",
    "assert buffer_tasks_dir.is_dir(), f\"buffer_tasks_dir does not exist.\\nbuffer_tasks_dir:\\n {buffer_tasks_dir}\"\n",
    "# agaid / offline_data / env_id / task_val / buffers\n",
    "\n",
    "pattern = re.compile(args.buffer_name_pattern)\n",
    "task_buffer_paths = [] # One name for each task\n",
    "for buffer_dir in buffer_tasks_dir.iterdir():\n",
    "    buffer_paths = []\n",
    "    for buffer_path in buffer_dir.iterdir():\n",
    "        if pattern.fullmatch(buffer_path.name): buffer_paths.append(buffer_path)\n",
    "    assert len(buffer_paths) == 1, f\"buffer_name_pattern must match exactly 1 buffer file per task subfolder \"\\\n",
    "        f\"but instead matched {len(buffer_paths)}.\\nbuffer_dir: {buffer_dir}\\nbuffer_paths: {buffer_paths}\"\n",
    "    task_buffer_paths += buffer_paths\n",
    "# print(f\"buffer_names:\", *(path.name for path in task_buffer_paths), sep='\\n')\n",
    "\n",
    "# data_sizes = np.flip(np.sort(np.geomspace(100000, 100, 10).astype(int))) # In descending order of size.\n",
    "data_sizes = np.flip(np.sort(np.geomspace(5000, 100, 2).astype(int))) # In descending order of size.\n",
    "\n",
    "# Check if CUDA is available (i.e., GPU is available)\n",
    "if torch.cuda.is_available(): device = torch.device(\"cuda\") # Use GPU\n",
    "else: device = torch.device(\"cpu\") # Use CPU\n",
    "\n",
    "# Load buffers for all tasks.\n",
    "task_buffers = []\n",
    "for task_buffer_path in tqdm(task_buffer_paths[:2]):\n",
    "    buffer = MT_Model_Buffer.load(task_buffer_path)\n",
    "    task_buffers.append(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17b6e97-b613-401c-ae38-2f6af6667f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shuffle_episodes(self):\n",
    "    \"\"\"\n",
    "    Shuffle episodes, i.e. trajectories.\n",
    "    \"\"\"\n",
    "    if not self._trimmed:\n",
    "        self._trim_buffer()\n",
    "    random.shuffle(self.tasks)\n",
    "    random.shuffle(self.states)\n",
    "    random.shuffle(self.actions)\n",
    "    random.shuffle(self.next_states)\n",
    "    random.shuffle(self.dones)\n",
    "\n",
    "from types import MethodType\n",
    "\n",
    "for task_buffer in task_buffers:\n",
    "    task_buffer.shuffle_episodes = MethodType(shuffle_episodes, task_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b82e11c-e9b9-4805-8e92-31038bdf5da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1001, 1001]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(b.states) for b in task_buffers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae51452a-dc78-4b1b-9318-e4b4ebc1b1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b.states is None for b in task_buffers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "485db6f1-7bbd-4b98-a87a-ec842ed95596",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from importlib import  import_module\n",
    "\n",
    "# import ipdb as pdb #  for debugging\n",
    "from ipdb import set_trace\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tyro\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "\n",
    "from mt_model_buffer import MT_Model_Buffer\n",
    "from nn.world_model import Dynamics_Model_Multihead, Dynamics_Model_Embed, Dynamics_Model_Aggregate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    env_id: str = \"Pendulum-v1\"\n",
    "    \"\"\"The id of the environment\"\"\"\n",
    "    exp_id: str = \"200k_0\"\n",
    "    \"\"\"The id of the experiment\"\"\"\n",
    "    batch_size: int = 128\n",
    "    \"\"\"The number of steps per mini-batch\"\"\"\n",
    "    epochs: int = 5\n",
    "    \"\"\"The number of epochs of training\"\"\"\n",
    "    lr: float = 1e-3\n",
    "    \"\"\"The number of epochs of training\"\"\"\n",
    "    buffer_name_pattern: str = \"min_total_steps_100000__actor__[\\w-]+\\.pkl\"\n",
    "    # buffer_name_pattern: str = \"min_total_steps_100000__actor__Pendulum-v1__td3_continuous_action__task_g_0.0_1__1713749400\"\n",
    "    \"\"\"The regex pattern for the unique buffer to load per task\"\"\"\n",
    "    # save_interval: int = 1000\n",
    "    # \"\"\"The number of epochs of training\"\"\"\n",
    "    # task_distribution: int = 1000\n",
    "    # \"\"\"The number of epochs of training\"\"\"\n",
    "    # cuda: bool = True\n",
    "    # \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    \n",
    "    # num_workers: int = 1\n",
    "    # \"\"\"the number of ray workers\"\"\"\n",
    "    # num_envs: int = 1\n",
    "    # \"\"\"the number of parallel game environments\"\"\"\n",
    "    # buffer_size: int = int(1e6)\n",
    "    # \"\"\"the replay memory buffer size\"\"\"\n",
    "\n",
    "\n",
    "def train_model_offline(exp_name, env_id, model_type, buffer, lr, epochs, batch_size, test_size=0.2, device=None):\n",
    "    \n",
    "    # Get obs_size and pred_size from env.\n",
    "    env = gym.make(env_id, render_mode=None)\n",
    "    env_obs_size = np.product(env.observation_space.shape)\n",
    "    env_act_size = np.product(env.action_space.shape)\n",
    "    assert len(env.observation_space.shape) == 1 == len(env.action_space.shape)\n",
    "    obs_size = env_obs_size + env_act_size\n",
    "    pred_size = env_obs_size\n",
    "    \n",
    "\n",
    "    logdir = Path('runs')\n",
    "    assert logdir.is_dir()\n",
    "    logdir /= env_id\n",
    "    logdir /= exp_name\n",
    "    logdir /= model_type\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    writer = SummaryWriter(logdir)\n",
    "    \n",
    "    num_tasks = len(np.unique(np.concatenate(buffer.tasks)))\n",
    "    if model_type == 'multihead':\n",
    "        model = Dynamics_Model_Multihead(obs_size=obs_size, pred_size=pred_size, num_tasks=num_tasks)\n",
    "        recurrent = False\n",
    "    if model_type == 'embed':\n",
    "        model = Dynamics_Model_Embed(obs_size=obs_size, pred_size=pred_size, num_tasks=num_tasks, embedding_dim=3)\n",
    "        recurrent = False\n",
    "    if model_type == 'aggregate':\n",
    "        model = Dynamics_Model_Aggregate(obs_size=obs_size, pred_size=pred_size)\n",
    "        recurrent = False\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unrecognized model_type: {model_type}\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Split buffer into train_buffer and test_buffer.\n",
    "    train_buffer, test_buffer = buffer.train_test_split(test_size=test_size)\n",
    "\n",
    "    # Train model, write logging info.\n",
    "    best_avg_loss = torch.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Train model for one epoch.\n",
    "        task_to_losses, avg_losses_across_tasks = train_model_offline_epoch(model=model, recurrent=recurrent, buffer=train_buffer, batch_size=batch_size, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Save updated model.\n",
    "        if avg_losses_across_tasks.mean() < best_avg_loss:\n",
    "            model.save(logdir / model_type)\n",
    "        \n",
    "        # Log training info.\n",
    "        log_train_test_info(writer=writer, train_or_test='train', task_to_losses=task_to_losses, avg_losses_across_tasks=avg_losses_across_tasks, epoch=epoch)\n",
    "\n",
    "        # Test model for one epoch.\n",
    "        task_to_losses, avg_losses_across_tasks = train_model_offline_epoch(model=model, recurrent=recurrent, buffer=test_buffer, batch_size=batch_size, optimizer=None, device=device)\n",
    "        \n",
    "        # Log testing info.\n",
    "        log_train_test_info(writer=writer, train_or_test='test', task_to_losses=task_to_losses, avg_losses_across_tasks=avg_losses_across_tasks, epoch=epoch)\n",
    "\n",
    "\n",
    "def log_train_test_info(writer, train_or_test, task_to_losses, avg_losses_across_tasks, epoch):\n",
    "    assert train_or_test in ['train', 'test']\n",
    "\n",
    "    # Write mean loss across all tasks and all state elements.\n",
    "    writer.add_scalar(f\"{train_or_test}/mean loss\", avg_losses_across_tasks.mean(), epoch)\n",
    "    # Write mean loss across all tasks for each state element.\n",
    "    for state_idx in range(len(avg_losses_across_tasks)):\n",
    "        writer.add_scalar(f\"{train_or_test}/state element {state_idx}/mean loss\", avg_losses_across_tasks[state_idx], epoch)\n",
    "    for task, task_losses in task_to_losses.items():\n",
    "        # Write mean loss for this task.\n",
    "        writer.add_scalar(f\"{train_or_test}/task losses/mean loss\", task_losses.mean(), epoch)\n",
    "        # Write avg loss for this task for each state element.\n",
    "        for state_idx in range(len(task_losses)):\n",
    "            writer.add_scalar(f\"{train_or_test}/task losses/task_{task}/state element {state_idx}\", task_losses[state_idx], epoch)\n",
    "\n",
    "\n",
    "def train_model_offline_epoch(model, recurrent, buffer, batch_size, optimizer=None, device=None):\n",
    "    \"\"\"optimizer=None deactivates training. This is used for testing.\"\"\"\n",
    "\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    for batch_idx, sample_batch in enumerate(buffer.sample(get_whole_trajectories=recurrent, batch_size=batch_size)):\n",
    "        for e in sample_batch:\n",
    "            e.to(device)\n",
    "        task_batch, state_batch, action_batch, next_state_batch, done_batch, not_padding_mask = sample_batch\n",
    "\n",
    "        # Normalize values.\n",
    "        state_batch_normalized = buffer.normalize_state(state_batch, inplace=False)\n",
    "        action_batch_normalized = buffer.normalize_action(action_batch, inplace=False)\n",
    "        next_state_batch_normalized = buffer.normalize_state(next_state_batch, inplace=False)\n",
    "\n",
    "        # Calculate residual.\n",
    "        delta_normalized_state_batch = next_state_batch_normalized - state_batch_normalized # The learning target.\n",
    "\n",
    "        # Get model prediction.\n",
    "        observation_batch = torch.cat((state_batch_normalized, action_batch_normalized), dim=-1)\n",
    "        predicted_normalized_residual_batch = model(observation_batch, task_batch) # prediction is predicted normalized next_state residual.\n",
    "\n",
    "        # Compute loss.\n",
    "        if optimizer is not None: optimizer.zero_grad()\n",
    "        # loss = F.mse_loss(input=predicted_normalized_residual_batch, target=delta_normalized_state_batch) # Doesn't account for not_padding_mask.\n",
    "        losses = ((predicted_normalized_residual_batch - delta_normalized_state_batch) * not_padding_mask).pow(2) # Shape: (batch_size, <max_traj_len if recurrent,> state_size).\n",
    "        loss = losses.sum() / not_padding_mask.sum()\n",
    "        if optimizer is not None:\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.01) #max_norm=grad_clip=?0.01\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_normalized_next_state_batch = predicted_normalized_residual_batch + state_batch_normalized\n",
    "            # prediction_denormalized_next_state_batch = buffer.denormalize_state(predicted_normalized_next_state_batch)\n",
    "            # normalized_mses = F.mse_loss(predicted_normalized_next_state_batch, next_state_batch_normalized, reduction='none')\n",
    "            avg_losses_across_tasks = losses.sum(dim=state_batch.shape()[:-1]) / not_padding_mask.sum(dim=state_batch.shape()[:-1]) # Shape: (state_size,).\n",
    "            # task_batch shape: (batch_size, <max_traj_len if recurrent,>, <possibly task_size if > 1,>).\n",
    "            task_to_losses = dict()\n",
    "            for task in task_batch.unique():\n",
    "                task_losses = losses[task_batch == task].mean(dim=0, dtype=float) # Shape: (state_size,).\n",
    "                # Averaged across batch, one element per state element per task.\n",
    "                task_to_losses[task] = task_losses\n",
    "            return task_to_losses, avg_losses_across_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a9192e7-407d-4101-b20f-a19bc7823d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82964536-9bc8-4850-b129-f6a975f593a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cat([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741c1734-9eea-4108-9b71-c09df2d28460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from mt_model_buffer import MT_Model_Buffer\n",
    "\n",
    "buffer = MT_Model_Buffer()\n",
    "for ep in range(200):\n",
    "    state = np.random.rand(5)\n",
    "    for step in range(100):\n",
    "        action = np.random.rand(2)\n",
    "        next_state = np.random.rand(5)\n",
    "        done = np.random.random() < 2/100\n",
    "        buffer.push(task=ep, state=state, action=action, next_state=next_state, done=done)\n",
    "        if done: break\n",
    "    buffer.end_trajectory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9183a478-7623-467b-b95c-53f74138046f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8900"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6785393f-22be-4b2c-a1a7-308177a7c06a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = buffer.get_subset_buffer(min_steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7de8b9d-fb89-493a-a10f-8207d9a84c72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split(self, test_size=None, train_size=None, shuffle=True, random_seed=None, view=True):\n",
    "    \"\"\"\n",
    "    Following some conventions of sklearn.model_selection.train_test_split:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "    test_size or train_size must be a float in [0, 1], indicating the proportion of elements in that partition.\n",
    "\n",
    "    The split is performed on whole trajectories, irrespective of their sizes.\n",
    "    TODO: make this true: The split is performed on whole trajectories to approximate the desired split on transitions.\n",
    "    TODO: support option to split on transitions.\n",
    "    \"\"\"\n",
    "    self._trim_buffer()\n",
    "    num_trajectories = len(self.states)\n",
    "    if shuffle: self.shuffle_episodes(seed=random_seed)\n",
    "\n",
    "    # Validate test_size and train_size to the proportion of steps in that partition.\n",
    "    if not ((test_size is None) ^ (train_size is None)): # If both or neither is None:\n",
    "        raise ValueError(\"Exactly one of test_size and train_size must be provided.\")\n",
    "    if train_size is not None:\n",
    "        # Infer test_size from train_size.\n",
    "        assert 0 <= train_size <= 1, f\"train_size must be in [0, 1]. train_size: {train_size}\"\n",
    "        test_size = 1 - train_size\n",
    "    else:\n",
    "        # Infer train_size from test_size.\n",
    "        assert 0 <= test_size <= 1, f\"test_size must be in [0, 1]. test_size: {test_size}\"\n",
    "        train_size = 1 - test_size\n",
    "    # train_size & test_size are in [0, 1].\n",
    "\n",
    "    # TODO: # Convert sizes to the number of steps, rounding towards 50-50.\n",
    "    # Convert sizes to the number of trajectories.\n",
    "    train_size = round(num_trajectories * train_size)\n",
    "    test_size = num_trajectories - train_size\n",
    "\n",
    "    # Perform partition.\n",
    "    train_test_split_buffers = [None, None]\n",
    "    for partition_idx, partition_size in enumerate([train_size, test_size]):\n",
    "        # Instantiate empty buffer.\n",
    "        train_test_split_buffers[partition_idx] = self.__class__()\n",
    "        train_test_split_buffers[partition_idx]._trim_buffer()\n",
    "        # Partition and load data into buffers.\n",
    "        for traj_data_name in ['tasks', 'states', 'actions', 'next_states', 'dones']:\n",
    "            traj_data = getattr(self, traj_data_name)\n",
    "            # Partition data.\n",
    "            traj_data_partition = traj_data[:partition_size]\n",
    "            # TODO: test whether view=True is broken upstream.\n",
    "            if not view: traj_data_partition = deepcopy(traj_data_partition)\n",
    "            # Load data.\n",
    "            setattr(train_test_split_buffers[partition_idx], traj_data_name, traj_data_partition)\n",
    "\n",
    "        # Restore buffer to a push-ready state.\n",
    "        train_test_split_buffers[partition_idx]._start_trajectory()\n",
    "        push_count = sum(map(lambda traj: len(traj), train_test_split_buffers[partition_idx].states))\n",
    "        train_test_split_buffers[partition_idx].size = push_count\n",
    "\n",
    "    train_buffer, test_buffer = train_test_split_buffers\n",
    "    return train_buffer, test_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62d90a25-eab6-443e-8aab-134edce82ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 15, 125, 15)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(sub, test_size=0.2, shuffle=False)\n",
    "train.size, test.size, sum(map(lambda e: len(e), train.states)), sum(map(lambda e: len(e), test.tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca24c0a6-8805-490f-bb5c-98468ad21062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2180, 898, 0.29)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = sub.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "train.size, test.size, round(test.size / (test.size + train.size), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b95d1-a465-4a6a-bdd9-655fa33d68d1",
   "metadata": {},
   "source": [
    "## Debug train_model_offline.py and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d8bee0-a6e7-4832-ad95-8723433c7394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/crowleyd/agaid/venv_agaid/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-02 01:27:26,232\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from importlib import  import_module\n",
    "\n",
    "# import ipdb as pdb #  for debugging\n",
    "from ipdb import set_trace\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tyro\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "\n",
    "from mt_model_buffer import MT_Model_Buffer\n",
    "from nn.world_model import Dynamics_Model_Multihead, Dynamics_Model_Embed, Dynamics_Model_Aggregate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    env_id: str = \"Pendulum-v1\"\n",
    "    \"\"\"The id of the environment\"\"\"\n",
    "    exp_id: str = \"200k_0\"\n",
    "    \"\"\"The id of the experiment\"\"\"\n",
    "    batch_size: int = 128\n",
    "    \"\"\"The number of steps per mini-batch\"\"\"\n",
    "    epochs: int = 2\n",
    "    \"\"\"The number of epochs of training\"\"\"\n",
    "    test_size: float = 0.2\n",
    "    \"\"\"The proportion of data kept out for testing\"\"\"\n",
    "    lr: float = 1e-3\n",
    "    \"\"\"The number of epochs of training\"\"\"\n",
    "    buffer_name_pattern: str = \"min_total_steps_100000__actor__[\\w-]+\\.pkl\"\n",
    "    # buffer_name_pattern: str = \"min_total_steps_100000__actor__Pendulum-v1__td3_continuous_action__task_g_0.0_1__1713749400\"\n",
    "    \"\"\"The regex pattern for the unique buffer to load per task\"\"\"\n",
    "    # save_interval: int = 1000\n",
    "    # \"\"\"The number of epochs of training\"\"\"\n",
    "    # task_distribution: int = 1000\n",
    "    # \"\"\"The number of epochs of training\"\"\"\n",
    "    # cuda: bool = True\n",
    "    # \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    \n",
    "    # num_workers: int = 1\n",
    "    # \"\"\"the number of ray workers\"\"\"\n",
    "    # num_envs: int = 1\n",
    "    # \"\"\"the number of parallel game environments\"\"\"\n",
    "    # buffer_size: int = int(1e6)\n",
    "    # \"\"\"the replay memory buffer size\"\"\"\n",
    "\n",
    "\n",
    "def train_model_offline(exp_name, env_id, model_type, buffer, lr, epochs, batch_size, test_size=0.2, device=None):\n",
    "    \n",
    "    # Get obs_size and pred_size from env.\n",
    "    env = gym.make(env_id, render_mode=None)\n",
    "    env_obs_size = np.prod(env.observation_space.shape)\n",
    "    env_act_size = np.prod(env.action_space.shape)\n",
    "    assert len(env.observation_space.shape) == 1 == len(env.action_space.shape)\n",
    "    obs_size = env_obs_size + env_act_size\n",
    "    pred_size = env_obs_size\n",
    "    \n",
    "\n",
    "    logdir = Path('runs')\n",
    "    assert logdir.is_dir()\n",
    "    logdir /= env_id\n",
    "    logdir /= exp_name\n",
    "    logdir /= model_type\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    writer = SummaryWriter(logdir)\n",
    "    \n",
    "    num_tasks = len(np.unique(np.concatenate(buffer.tasks)))\n",
    "    if model_type == 'multihead':\n",
    "        model = Dynamics_Model_Multihead(obs_size=obs_size, pred_size=pred_size, num_tasks=num_tasks)\n",
    "        recurrent = False\n",
    "    elif model_type == 'embed':\n",
    "        model = Dynamics_Model_Embed(obs_size=obs_size, pred_size=pred_size, num_tasks=num_tasks, embedding_dim=3)\n",
    "        recurrent = False\n",
    "    elif model_type == 'aggregate':\n",
    "        model = Dynamics_Model_Aggregate(obs_size=obs_size, pred_size=pred_size)\n",
    "        recurrent = False\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unrecognized model_type: {model_type}\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Split buffer into train_buffer and test_buffer.\n",
    "    train_buffer, test_buffer = buffer.train_test_split(test_size=test_size)\n",
    "\n",
    "    # Train model, write logging info.\n",
    "    best_avg_loss = torch.inf\n",
    "    for epoch in range(epochs):\n",
    "        # Train model for one epoch.\n",
    "        task_to_losses, avg_losses_across_tasks = train_model_offline_epoch(model=model, recurrent=recurrent, buffer=train_buffer, batch_size=batch_size, optimizer=optimizer, device=device)\n",
    "        \n",
    "        # Save updated model.\n",
    "        if avg_losses_across_tasks.mean() < best_avg_loss:\n",
    "            model.save(logdir / model_type)\n",
    "        \n",
    "        # Log training info.\n",
    "        log_train_test_info(writer=writer, train_or_test='train', task_to_losses=task_to_losses, avg_losses_across_tasks=avg_losses_across_tasks, epoch=epoch)\n",
    "\n",
    "        # Test model for one epoch.\n",
    "        task_to_losses, avg_losses_across_tasks = train_model_offline_epoch(model=model, recurrent=recurrent, buffer=test_buffer, batch_size=batch_size, optimizer=None, device=device)\n",
    "        \n",
    "        # Log testing info.\n",
    "        log_train_test_info(writer=writer, train_or_test='test', task_to_losses=task_to_losses, avg_losses_across_tasks=avg_losses_across_tasks, epoch=epoch)\n",
    "\n",
    "\n",
    "def log_train_test_info(writer, train_or_test, task_to_losses, avg_losses_across_tasks, epoch):\n",
    "    assert train_or_test in ['train', 'test']\n",
    "\n",
    "    # Write mean loss across all tasks and all state elements.\n",
    "    writer.add_scalar(f\"{train_or_test}/mean loss\", avg_losses_across_tasks.mean(), epoch)\n",
    "    # Write mean loss across all tasks for each state element.\n",
    "    for state_idx in range(len(avg_losses_across_tasks)):\n",
    "        writer.add_scalar(f\"{train_or_test}/state element {state_idx}/mean loss\", avg_losses_across_tasks[state_idx], epoch)\n",
    "    for task, task_losses in task_to_losses.items():\n",
    "        # Write mean loss for this task.\n",
    "        writer.add_scalar(f\"{train_or_test}/task losses/mean loss\", task_losses.mean(), epoch)\n",
    "        # Write avg loss for this task for each state element.\n",
    "        for state_idx in range(len(task_losses)):\n",
    "            writer.add_scalar(f\"{train_or_test}/task losses/task_{task}/state element {state_idx}\", task_losses[state_idx], epoch)\n",
    "\n",
    "\n",
    "def train_model_offline_epoch(model, recurrent, buffer, batch_size, optimizer=None, device=None):\n",
    "    \"\"\"optimizer=None deactivates training. This is used for testing.\"\"\"\n",
    "\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    for batch_idx, sample_batch in enumerate(buffer.sample(get_whole_trajectories=recurrent, batch_size=batch_size)):\n",
    "        for e in sample_batch:\n",
    "            e.to(device)\n",
    "        task_batch, state_batch, action_batch, next_state_batch, done_batch, not_padding_mask = sample_batch\n",
    "\n",
    "        # Normalize values.\n",
    "        state_batch_normalized = buffer.normalize_state(state_batch, inplace=False)\n",
    "        action_batch_normalized = buffer.normalize_action(action_batch, inplace=False)\n",
    "        next_state_batch_normalized = buffer.normalize_state(next_state_batch, inplace=False)\n",
    "\n",
    "        # Calculate residual.\n",
    "        delta_normalized_state_batch = next_state_batch_normalized - state_batch_normalized # The learning target.\n",
    "\n",
    "        # Get model prediction.\n",
    "        observation_batch = torch.cat((state_batch_normalized, action_batch_normalized), dim=-1)\n",
    "        predicted_normalized_residual_batch = model(observation_batch, task_batch) # prediction is predicted normalized next_state residual.\n",
    "\n",
    "        # Compute loss.\n",
    "        if optimizer is not None: optimizer.zero_grad()\n",
    "        # loss = F.mse_loss(input=predicted_normalized_residual_batch, target=delta_normalized_state_batch) # Doesn't account for not_padding_mask.\n",
    "        losses = ((predicted_normalized_residual_batch - delta_normalized_state_batch) * not_padding_mask).pow(2) # Shape: (batch_size, <max_traj_len if recurrent,> state_size).\n",
    "        loss = losses.sum() / not_padding_mask.sum()\n",
    "        if optimizer is not None:\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.01) #max_norm=grad_clip=?0.01\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_normalized_next_state_batch = predicted_normalized_residual_batch + state_batch_normalized\n",
    "            # prediction_denormalized_next_state_batch = buffer.denormalize_state(predicted_normalized_next_state_batch)\n",
    "            # normalized_mses = F.mse_loss(predicted_normalized_next_state_batch, next_state_batch_normalized, reduction='none')\n",
    "            avg_losses_across_tasks = losses.sum(dim=state_batch.shape()[:-1]) / not_padding_mask.sum(dim=state_batch.shape()[:-1]) # Shape: (state_size,).\n",
    "            # task_batch shape: (batch_size, <max_traj_len if recurrent,>, <possibly task_size if > 1,>).\n",
    "            task_to_losses = dict()\n",
    "            for task in task_batch.unique():\n",
    "                task_losses = losses[task_batch == task].mean(dim=0, dtype=float) # Shape: (state_size,).\n",
    "                # Averaged across batch, one element per state element per task.\n",
    "                task_to_losses[task] = task_losses\n",
    "            return task_to_losses, avg_losses_across_tasks\n",
    "\n",
    "args = Args()\n",
    "\n",
    "path_to_agaid = Path.cwd()\n",
    "assert path_to_agaid.name == \"agaid\"\n",
    "buffer_tasks_dir = path_to_agaid / f\"offline_data/{args.env_id}\"\n",
    "assert buffer_tasks_dir.is_dir(), f\"buffer_tasks_dir does not exist.\\nbuffer_tasks_dir:\\n {buffer_tasks_dir}\"\n",
    "# agaid / offline_data / env_id / task_val / buffers\n",
    "\n",
    "pattern = re.compile(args.buffer_name_pattern)\n",
    "task_buffer_paths = [] # One name for each task\n",
    "for buffer_dir in buffer_tasks_dir.iterdir():\n",
    "    buffer_paths = []\n",
    "    for buffer_path in buffer_dir.iterdir():\n",
    "        if pattern.fullmatch(buffer_path.name): buffer_paths.append(buffer_path)\n",
    "    assert len(buffer_paths) == 1, f\"buffer_name_pattern must match exactly 1 buffer file per task subfolder \"\\\n",
    "        f\"but instead matched {len(buffer_paths)}.\\nbuffer_dir: {buffer_dir}\\nbuffer_paths: {buffer_paths}\"\n",
    "    task_buffer_paths += buffer_paths\n",
    "# print(f\"buffer_names:\", *(path.name for path in task_buffer_paths), sep='\\n') # debug\n",
    "\n",
    "# data_sizes = np.flip(np.sort(np.geomspace(100000, 100, 10).astype(int))) # In descending order of size.\n",
    "data_sizes = np.flip(np.sort(np.geomspace(5000, 100, 2).astype(int))) # In descending order of size. #  TODO:  -------------------------->RESTORE   TODO\n",
    "\n",
    "# Check if CUDA is available (i.e., GPU is available)\n",
    "if torch.cuda.is_available(): device = torch.device(\"cuda\") # Use GPU\n",
    "else: device = torch.device(\"cpu\") # Use CPU\n",
    "\n",
    "# Load buffers for all tasks.\n",
    "task_buffers = []\n",
    "for task_buffer_path in task_buffer_paths[:2]: #  TODO:  -------------------------->RESTORE   TODO\n",
    "    buffer = MT_Model_Buffer.load(task_buffer_path, reinstantiate=True)\n",
    "    task_buffers.append(buffer)\n",
    "# print(\"buffer sizes:\", *(buffer.size for buffer in task_buffers), sep='\\n') # debug\n",
    "# print(\"buffer manual sizes:\", *(sum(map(lambda e: len(e), buffer.states)) for buffer in task_buffers), sep='\\n') # debug\n",
    "# print(\"buffer num eps:\", *(len(buffer.states) for buffer in task_buffers), sep='\\n') # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7916fea-a7c4-4fe1-b425-7850552fab31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 500 100000\n",
      "100000 500 100000\n"
     ]
    }
   ],
   "source": [
    "for tb in task_buffers:\n",
    "    print(tb.size, len(tb.states), sum(map(lambda e: len(e), tb.states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1cf772-8405-461d-bd38-0b260bfb0d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data_size in data_sizes:\n",
    "    # Subsample and combine buffers.\n",
    "    # Note: these methods may share buffer memory.\n",
    "    sub_task_buffers = [task_buffer.get_subset_buffer(min_steps=data_size, no_extra=True) for task_buffer in task_buffers]\n",
    "    buffer = MT_Model_Buffer.merge_buffers(sub_task_buffers)\n",
    "    buffer.shuffle_episodes()\n",
    "\n",
    "    break\n",
    "    # Train a model for each model_type.\n",
    "    for model_type in ['multihead', 'embed', 'aggregate']:\n",
    "        print(f\"Beginning to train model_type {model_type}\") # debug\n",
    "        train_model_offline(exp_name=f\"{args.exp_id}/data_size_{data_size}\", env_id=args.env_id, model_type=model_type, \\\n",
    "                            buffer=buffer, lr=args.lr, epochs=args.epochs, batch_size=args.batch_size, test_size=args.test_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7aaf4f-5015-4067-a1f7-35131623ba7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_buffer, test_buffer = buffer.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9e9511-89d7-4ea4-827e-4651497c8baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for b_idx, b_sample in enumerate(train_buffer.sample(get_whole_trajectories=False, batch_size=args.batch_size)):\n",
    "    for e in b_sample:\n",
    "        # print(e)\n",
    "        e.to('cpu')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6604ecb-18b6-4cbf-b905-da2cb275eec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.5983, 6.5983, 6.5983], dtype=torch.float64),\n",
       " tensor([ 8.0377,  9.0377, 10.0377], dtype=torch.float64))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [torch.arange(np.random.randint(1, 10)*3).reshape(-1, 3).to(float) for i in range(10)]\n",
    "torch.std_mean(torch.from_numpy(np.concatenate(x)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72242b30-9303-4ca5-8eee-d45204cccf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "from_numpy(ndarray) -> Tensor\n",
       "\n",
       "Creates a :class:`Tensor` from a :class:`numpy.ndarray`.\n",
       "\n",
       "The returned tensor and :attr:`ndarray` share the same memory. Modifications to\n",
       "the tensor will be reflected in the :attr:`ndarray` and vice versa. The returned\n",
       "tensor is not resizable.\n",
       "\n",
       "It currently accepts :attr:`ndarray` with dtypes of ``numpy.float64``,\n",
       "``numpy.float32``, ``numpy.float16``, ``numpy.complex64``, ``numpy.complex128``,\n",
       "``numpy.int64``, ``numpy.int32``, ``numpy.int16``, ``numpy.int8``, ``numpy.uint8``,\n",
       "and ``bool``.\n",
       "\n",
       ".. warning::\n",
       "    Writing to a tensor created from a read-only NumPy array is not supported and will result in undefined behavior.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = numpy.array([1, 2, 3])\n",
       "    >>> t = torch.from_numpy(a)\n",
       "    >>> t\n",
       "    tensor([ 1,  2,  3])\n",
       "    >>> t[0] = -1\n",
       "    >>> a\n",
       "    array([-1,  2,  3])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??torch.from_numpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agaid_kernel",
   "language": "python",
   "name": "venv_agaid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
